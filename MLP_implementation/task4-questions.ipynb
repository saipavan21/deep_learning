{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM4040 Assignment 1, Task 4: Questions\n",
    "\n",
    "1) What is the difference between the SVM method and a neural network, assuming that both work with the same number of training samples N?\n",
    "\n",
    "   Your answer: \n",
    "       1. NN are more tend to finding the local minima, in contrast SVM finds global minima.\n",
    "       2. SVMs are less prone to overfitting as they deal with less parameters whereas incase of NN, they are more prone to overfitting.\n",
    "       3. Training a NN requires a tedious process with lot of tuning the hyperparameters.\n",
    "       4. NN requires huge memory and comupationally not efficient when compared to SVM.\n",
    "   \n",
    "\n",
    "\n",
    "2) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer:\n",
    "       1. Relu activation is computationally very efficient as simple thresholding is required.\n",
    "       2. It can be implemented simply by thresholding the activation matrix at 0, whereas other non-linear activation function like sigmoid/tanh requires expensive operations like exponentials.\n",
    "       3. Gradient wonâ€™t saturate in the positive region.\n",
    "       4. Incase of sigmoid/tanh activation functions, the bigger the input the smaller the gradient. And the gradient of sigmoid will always be smaller than one.For deep learning with many layers gradients will vanish as we move deep in to layers. In contrast, the gradients of the Relu function is either 0 for input<0 or 1 for input >0. So, the gradient will neither vanish nor explode as we move deep into layers. The constant gradient of Relu results in faster learning.\n",
    "       5. It induces sparsity by reducing the memory, computation time, and also helps in reducing the overfitting.\n",
    "   \n",
    "\n",
    "3) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  the hyperparameters, which stategies you did you use to improve the network, show the results of intermediate and final steps.\n",
    "\n",
    "   Your answer:\n",
    "       1. starting point = {hidden_dim = 250, reg=0.1, weight_scale= 1e-1, epoch =10, optim = 'SGD', batch_size = 500} got the accuracy of 20.1%\n",
    "       2. And increased the number of epoch to 20 with other hyperparameters as same got an accuracy of 30.4%\n",
    "       3. As the hidden dimension is huge, tried reducing the weight scale to initialize small weights so that some neurons will be dead and get the sparse network.\n",
    "       4. Reduced the weight scale to 1e-2 with epoch =20 and rest of parameters same got accuracy of 49.7%\n",
    "       5. Further reduced the weight scale to 1e-3 to get more sparse network and got accuracy of 52.03%\n",
    "       6. As there was gap between validation accuracy and testing accuracy so increased the regularization parameter from 0.1 to 0.2 and got accuracy 52.3%\n",
    "       7. used SGD with momentum instead of SGD and got accuracy of 53.3%\n",
    "       8. Final point = {hidden_dim = 250, reg= 0.2, weight_scale = 1e-3, optim='SGD with Momentum', batch_size=500} with accuracy 53.3%\n",
    "   \n",
    "\n",
    "4) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation** if you want to use it to find a best set of hyperparameter of the **Linear SVM classification** problem.\n",
    "\n",
    "   Your answer:\n",
    "       1. Divide the training data roughly into  K equal parts.\n",
    "       2. Select one part among k parts as validation set.\n",
    "       3. Train the model using remaining k-1 parts with hyperparameter 'lambda'.\n",
    "       4. Compute the error in predicting validation set.\n",
    "       5. Repeat steps 2,3,4 by selecting a different validation set everytime with same hyperparameter.\n",
    "       6. Compute the mean validation error.\n",
    "       7. Repeat steps from 1-6 with diffrent hyperparameters.\n",
    "       8. Select the hyperparameter which has the least mean validation error.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
