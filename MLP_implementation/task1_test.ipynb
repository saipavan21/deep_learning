{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from ecbm4040.cifar_utils import load_data\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 26,  17,  13, ...,  27,  26,  27],\n",
       "       [ 94, 101,  95, ..., 182, 184, 155],\n",
       "       [183, 158, 166, ..., 250, 250, 250],\n",
       "       ..., \n",
       "       [225, 214, 190, ..., 144, 167, 171],\n",
       "       [ 82,  69,  63, ...,  67,  57,  68],\n",
       "       [198, 173, 144, ...,  40,  31,  26]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "X_test = X_test[:num_test, :]\n",
    "y_test = y_test[:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 3073) (1000, 3073) (1000, 3073) (100, 3073)\n"
     ]
    }
   ],
   "source": [
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "# Append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2321)\n",
    "W = np.random.randn(3073, 10) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Multi-class Linear SVM loss function, naive implementation (with loops).\n",
    "    \n",
    "    In default, delta is 1 and there is no penalty term wst delta in objective function.\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: a numpy array of shape (D, C) containing weights.\n",
    "    - X: a numpy array of shape (N, D) containing N samples.\n",
    "    - y: a numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "         that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) L2 regularization strength\n",
    "\n",
    "    Returns:\n",
    "    - loss: a float scalar\n",
    "    - gradient: wrt weights W, an array of same shape as W\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape).astype('float') # initialize the gradient as zero\n",
    "\n",
    "    # compute the loss and the gradient\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "            margin = scores[j] - correct_class_score + 1 # note delta = 1\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[:,j] += X[i]\n",
    "                dW[:,y[i]] -= X[i]\n",
    "\n",
    "    # Right now the loss is a sum over all training examples, but we want it\n",
    "    # to be an average instead so we divide by num_train.\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "    # Add regularization to the loss.\n",
    "    loss += reg * np.sum(W * W)\n",
    "    dW += reg*2*W\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive numpy loss: 9.012755325014963, takes 0.08477616310119629 seconds.\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dw = np.zeros(W.shape).astype('float')\n",
    "    num_train = X.shape[0]\n",
    "    scores = X.dot(W)\n",
    "    correct_class_score = []\n",
    "    for j in range(num_train):\n",
    "        correct_class_score.append(-1 * scores[j][y[j]])\n",
    "    correct_class_score = np.array(correct_class_score)\n",
    "    correct_class_score += 1\n",
    "    margin = np.transpose(np.add(np.transpose(scores), correct_class_score))\n",
    "    margin = margin.clip(min=0)\n",
    "    loss = np.sum(margin) - num_train\n",
    "    loss /= num_train\n",
    "    loss += reg * np.sum(W * W)\n",
    "    return loss\n",
    "\n",
    "def svm_loss_vec(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dw = np.zeros(W.shape).astype('float')\n",
    "    num_train = X.shape[0]\n",
    "    scores = X.dot(W)\n",
    "    margin = (np.maximum(0, scores.transpose()-scores[np.arange(num_train),y] +1)).transpose()\n",
    "    margin[np.arange(len(margin)), y] = 0\n",
    "    loss = np.sum(margin)\n",
    "    loss /= num_train\n",
    "    loss += reg * np.sum(W * W)\n",
    "    \n",
    "    pos_margin = margin\n",
    "    pos_margin[margin >0] = 1\n",
    "    n_pos_row = np.sum(pos_margin, axis=1)\n",
    "    pos_margin[np.arange(num_train), y] = -1 * n_pos_row.transpose()\n",
    "    dw = X.transpose().dot(pos_margin)\n",
    "    dw /= num_train\n",
    "    dw += reg*2*W\n",
    "    \n",
    "    return loss, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized numpy loss: 9.012755325014968, takes 0.005579948425292969 seconds.\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_vec = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized numpy loss: 9.012755325014968, takes 0.005389213562011719 seconds.\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_vec, grad_vec = svm_loss_vec(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is vectorized loss correct? True\n",
      "Is vectorized gradient correct? True\n"
     ]
    }
   ],
   "source": [
    "print('Is vectorized loss correct? {}'.format(np.allclose(loss_naive, loss_vec)))\n",
    "print('Is vectorized gradient correct? {}'.format(np.allclose(grad_naive, grad_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow loss: 2.356956720352173, and tesnsorflow gradient: [array([[-0.91497672, -5.06715679, -1.63596535, ...,  1.62781715,\n",
      "        -1.5216608 ,  1.39993322],\n",
      "       [-0.89814317, -4.64034128, -1.64831865, ...,  1.119488  ,\n",
      "        -1.64277577,  1.39054656],\n",
      "       [-0.58115494, -5.12523413, -1.36927342, ...,  0.8880381 ,\n",
      "        -1.87494397,  1.08563268],\n",
      "       ..., \n",
      "       [-1.4052726 , -1.3936727 ,  2.4050858 , ...,  0.24068165,\n",
      "        -2.01775455,  0.46139181],\n",
      "       [-1.76994145, -0.95524085,  1.54262614, ..., -0.24725348,\n",
      "        -1.54199946,  0.30317867],\n",
      "       [-0.02036226, -0.02305621,  0.02698388, ...,  0.02119692,\n",
      "        -0.0108738 ,  0.01071431]], dtype=float32)], takes 0.33017420768737793 seconds\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2321)\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "\n",
    "# ground truth of loss and gradient\n",
    "W_tf = tf.placeholder(tf.float32, shape=(3073,10))\n",
    "X = tf.placeholder(tf.float32, shape=(None, 3073))\n",
    "y = tf.placeholder(tf.int32, shape=(None,))\n",
    "reg = tf.constant(0.000005)\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= tf.matmul(X, W_tf), labels=tf.one_hot(y,10))\n",
    "loss0 = tf.reduce_mean(cross_entropy) + reg*tf.reduce_sum(W_tf*W_tf)\n",
    "grad0 = tf.gradients(loss0, W_tf)\n",
    "out0 = (loss0, grad0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    tic = time.time()\n",
    "    loss_gt, grad_gt = sess.run(out0, feed_dict={W_tf: W, X: X_dev, y: y_dev})\n",
    "    toc = time.time()\n",
    "    print(\"tensorflow loss: {}, and tesnsorflow gradient: {}, takes {} seconds\".format(loss_gt, grad_gt, toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_vec(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    num_train = X.shape[0]\n",
    "    scores = X.dot(W)\n",
    "    scores -= np.amax(scores, axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(scores)\n",
    "    sigma_scores = exp_scores/ np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    req_sigma_scores = sigma_scores[np.arange(num_train), y]\n",
    "    loss = np.sum(-np.log(req_sigma_scores))\n",
    "    loss /= num_train\n",
    "    loss += reg * np.sum(W * W)\n",
    "    \n",
    "    Z_gradient = sigma_scores\n",
    "    Z_gradient[np.arange(num_train), y] -= 1\n",
    "    dW = X.transpose().dot(Z_gradient)\n",
    "    dW /= num_train\n",
    "    dW += reg * 2 * W\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized numpy loss: 2.356956723546791, takes 0.004266262054443359 seconds.\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "soft_loss_vec, soft_grad_vec = softmax_loss_vec(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(soft_loss_vec, toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    num_train = X.shape[0]\n",
    "    num_classes = W.shape[1]\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        scores -= np.max(scores)\n",
    "        exp_scores = np.exp(scores)\n",
    "        sigma_score = exp_scores/np.sum(exp_scores)\n",
    "        loss -= np.log(sigma_score[y[i]])\n",
    "        sigma_score[y[i]] -= 1\n",
    "        for j in range(num_classes):\n",
    "            dW[:,j] += (X[i] * sigma_score[j])\n",
    "    \n",
    "    loss /= num_train\n",
    "    loss += reg* np.sum(W*W)\n",
    "    \n",
    "    dW /= num_train\n",
    "    dW += reg * 2 * W\n",
    "    \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive numpy loss: 2.3569567235467903, takes 0.08016157150268555 seconds.\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "soft_loss_naive, soft_grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(soft_loss_naive, toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient error of naive softmax is 3.211899978540529e-07\n"
     ]
    }
   ],
   "source": [
    "print('Gradient error of naive softmax is {}'.format(rel_err(grad_gt,soft_grad_naive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  9.08963726e-04,   1.44710292e+00],\n",
       "        [ -9.15731138e-01,  -4.24139151e-01]],\n",
       "\n",
       "       [[  8.13202508e-02,   7.68532526e-02],\n",
       "        [  2.26363056e-01,  -2.21916469e+00]],\n",
       "\n",
       "       [[ -2.17271120e+00,  -3.27054119e-01],\n",
       "        [  2.88456776e-01,  -1.55368269e+00]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(a.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.reshape(a, [a.shape[0], np.prod(a.shape[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.08963726e-04,   1.44710292e+00,  -9.15731138e-01,\n",
       "         -4.24139151e-01],\n",
       "       [  8.13202508e-02,   7.68532526e-02,   2.26363056e-01,\n",
       "         -2.21916469e+00],\n",
       "       [ -2.17271120e+00,  -3.27054119e-01,   2.88456776e-01,\n",
       "         -1.55368269e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
