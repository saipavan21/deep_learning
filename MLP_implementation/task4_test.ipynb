{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation implementation:\n",
    "    1. Divide the training data roughly into  K equal parts\n",
    "    2. Select one part among k parts as validation set\n",
    "    3. Train the model using remaining k-1 parts with hyperparameter 'lambda'\n",
    "    4. Compute the error in predicting validation set\n",
    "    5. Repeat steps 2,3,4 by selecting a different validation set everytime with same hyperparameter.\n",
    "    6. Compute the mean validation error.\n",
    "    7. Repeat steps from 1-6 with diffrent hyperparameters\n",
    "    8. Select the hyperparameter which has the least mean validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu:\n",
    "    1. Relu activation is computationally very efficient as simple thresholding is required.\n",
    "    2. It can be implemented simply by thresholding the activation matrix at 0, whereas other non-linear activation function like sigmoid/tanh requires expensive operations like exponentials.\n",
    "    3. Gradient wonâ€™t saturate in the positive region.\n",
    "    4. Incase of sigmoid/tanh activation functions, the bigger the input the smaller the gradient. And the gradient of sigmoid will always be smaller than one.For deep learning with many layers gradients will vanish as we move deep in to layers. In contrast, the gradients of the Relu function is either 0 for input<0 or 1 for input >0. So, the gradient will neither vanish nor explode as we move deep into layers. The constant gradient of Relu results in faster learning.\n",
    "    5. It induces sparsity by reducing the memory, computation time, and also helps in reducing the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM vs NN:\n",
    "    1. NN requires huge memory and comupationally not efficient when compared to SVM\n",
    "    2. Training a nn requires a tedious process by experimenting with many hyperparameters\n",
    "    3. SVMs are less prone to overfitting as they deal with less parameters whereas incase of NN, they are more prone to overfitting.\n",
    "    4. NN are more tend to finding the local minima, in contrast SVM finds global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
