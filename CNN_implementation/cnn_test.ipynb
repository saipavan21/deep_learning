{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ecbm4040.cifar_utils import load_data\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/cifar-10-python.tar.gz already exists. Begin extracting...\n",
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train = load_data(mode='train')\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49000\n",
    "# Validation data: 1000 samples from original train set: 49000~50000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data, and reshape it to be RGB size\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "X_train = X_train.reshape([-1,32,32,3])/255\n",
    "X_val = X_val.reshape([-1,32,32,3])/255\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class norm_layer(object):\n",
    "    def __init__(self, input_x, isTrain):\n",
    "        \"\"\"\n",
    "        :param input_x: The input that needed for normalization.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('batch_norm'):\n",
    "            mean, variance = tf.nn.moments(input_x, axes=[0], keep_dims=True)\n",
    "            cell_out = tf.nn.batch_normalization(input_x,mean,variance,offset=None,\n",
    "                                                 scale=None,variance_epsilon=1e-6,name=None)\n",
    "            #cell_out = tf.layers.batch_normalization(input_x, training=isTrain)\n",
    "            self.cell_out = cell_out\n",
    "\n",
    "    def output(self):\n",
    "        return self.cell_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropout_layer(object):\n",
    "    def __init__(self, input_x, keep_prob):\n",
    "        \"\"\"\n",
    "        :param input_x: The input that needed for dropout.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('dropout'):\n",
    "            cell_out = tf.nn.dropout(input_x, keep_prob)\n",
    "            self.cell_out = cell_out\n",
    "            \n",
    "    def output(self):\n",
    "        return self.cell_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_layer(object):\n",
    "    def __init__(self, input_x, in_channel, out_channel, kernel_shape, rand_seed,index=0, isTrain=True):\n",
    "        \"\"\"\n",
    "        :param input_x: The input of the conv layer. Should be a 4D array like (batch_num, img_len, img_len, channel_num)\n",
    "        :param in_channel: The 4-th demension (channel number) of input matrix. For example, in_channel=3 means the input contains 3 channels.\n",
    "        :param out_channel: The 4-th demension (channel number) of output matrix. For example, out_channel=5 means the output contains 5 channels (feature maps).\n",
    "        :param kernel_shape: the shape of the kernel. For example, kernal_shape = 3 means you have a 3*3 kernel.\n",
    "        :param rand_seed: An integer that presents the random seed used to generate the initial parameter value.\n",
    "        :param index: The index of the layer. It is used for naming only.\n",
    "        \"\"\"\n",
    "        assert len(input_x.shape) == 4 and input_x.shape[1] == input_x.shape[2] and input_x.shape[3] == in_channel\n",
    "\n",
    "        with tf.variable_scope('conv_layer_%d' % index):\n",
    "            with tf.name_scope('conv_kernel'):\n",
    "                w_shape = [kernel_shape, kernel_shape, in_channel, out_channel]\n",
    "                weight = tf.get_variable(name='conv_kernel_%d' % index, shape=w_shape,\n",
    "                                         initializer=tf.glorot_uniform_initializer(seed=rand_seed))\n",
    "                self.weight = weight\n",
    "\n",
    "            with tf.variable_scope('conv_bias'):\n",
    "                b_shape = [out_channel]\n",
    "                bias = tf.get_variable(name='conv_bias_%d' % index, shape=b_shape,\n",
    "                                       initializer=tf.glorot_uniform_initializer(seed=rand_seed))\n",
    "                self.bias = bias\n",
    "\n",
    "            # strides [1, x_movement, y_movement, 1]\n",
    "            conv_out = tf.nn.conv2d(input_x, weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "            cell_out = tf.nn.relu(norm_layer(conv_out + bias, isTrain).output())\n",
    "\n",
    "            self.cell_out = cell_out\n",
    "\n",
    "            tf.summary.histogram('conv_layer/{}/kernel'.format(index), weight)\n",
    "            tf.summary.histogram('conv_layer/{}/bias'.format(index), bias)\n",
    "\n",
    "    def output(self):\n",
    "        return self.cell_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pooling_layer(object):\n",
    "    def __init__(self, input_x, k_size, padding=\"SAME\"):\n",
    "        \"\"\"\n",
    "        :param input_x: The input of the pooling layer.\n",
    "        :param k_size: The kernel size you want to behave pooling action.\n",
    "        :param padding: The padding setting. Read documents of tf.nn.max_pool for more information.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('max_pooling'):\n",
    "            # strides [1, k_size, k_size, 1]\n",
    "            pooling_shape = [1, k_size, k_size, 1]\n",
    "            cell_out = tf.nn.max_pool(input_x, strides=pooling_shape,\n",
    "                                      ksize=pooling_shape, padding=padding)\n",
    "            self.cell_out = cell_out\n",
    "\n",
    "    def output(self):\n",
    "        return self.cell_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer(object):\n",
    "    def __init__(self, input_x, in_size, out_size, rand_seed, activation_function=None, index=0, isTrain=True):\n",
    "        \"\"\"\n",
    "        :param input_x: The input of the FC layer. It should be a flatten vector.\n",
    "        :param in_size: The length of input vector.\n",
    "        :param out_size: The length of output vector.\n",
    "        :param rand_seed: An integer that presents the random seed used to generate the initial parameter value.\n",
    "        :param keep_prob: The probability of dropout. Default set by 1.0 (no drop-out applied)\n",
    "        :param activation_function: The activation function for the output. Default set to None.\n",
    "        :param index: The index of the layer. It is used for naming only.\n",
    "\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('fc_layer_%d' % index):\n",
    "            with tf.name_scope('fc_kernel'):\n",
    "                w_shape = [in_size, out_size]\n",
    "                weight = tf.get_variable(name='fc_kernel_%d' % index, shape=w_shape,\n",
    "                                         initializer=tf.glorot_uniform_initializer(seed=rand_seed))\n",
    "                self.weight = weight\n",
    "\n",
    "            with tf.variable_scope('fc_kernel'):\n",
    "                b_shape = [out_size]\n",
    "                bias = tf.get_variable(name='fc_bias_%d' % index, shape=b_shape,\n",
    "                                       initializer=tf.glorot_uniform_initializer(seed=rand_seed))\n",
    "                self.bias = bias\n",
    "\n",
    "            cell_out = tf.add(tf.matmul(input_x, weight), bias)\n",
    "            if activation_function is not None:\n",
    "                cell_out = activation_function(norm_layer(cell_out, isTrain).output())\n",
    "\n",
    "            self.cell_out = cell_out\n",
    "\n",
    "            tf.summary.histogram('fc_layer/{}/kernel'.format(index), weight)\n",
    "            tf.summary.histogram('fc_layer/{}/bias'.format(index), bias)\n",
    "\n",
    "    def output(self):\n",
    "        return self.cell_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet(input_x, input_y,\n",
    "          img_len=32, channel_num=3, output_size=10,\n",
    "          conv_featmap=[6, 16], fc_units=[84],\n",
    "          conv_kernel_size=[5, 5], pooling_size=[2, 2],\n",
    "          l2_norm=0.01, seed=235, isTrain= True, keep_prob=1):\n",
    "    \"\"\"\n",
    "        LeNet is an early and famous CNN architecture for image classfication task.\n",
    "        It is proposed by Yann LeCun. Here we use its architecture as the startpoint\n",
    "        for your CNN practice. Its architecture is as follow.\n",
    "\n",
    "        input >> Conv2DLayer >> Conv2DLayer >> flatten >>\n",
    "        DenseLayer >> AffineLayer >> softmax loss >> output\n",
    "\n",
    "        Or\n",
    "\n",
    "        input >> [conv2d-maxpooling] >> [conv2d-maxpooling] >> flatten >>\n",
    "        DenseLayer >> AffineLayer >> softmax loss >> output\n",
    "\n",
    "        http://deeplearning.net/tutorial/lenet.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(conv_featmap) == len(conv_kernel_size) and len(conv_featmap) == len(pooling_size)\n",
    "\n",
    "    # conv layer\n",
    "    \n",
    "    conv_layer_0 = conv_layer(input_x=input_x,\n",
    "                              in_channel=channel_num,\n",
    "                              out_channel=conv_featmap[0],\n",
    "                              kernel_shape=conv_kernel_size[0],\n",
    "                              rand_seed=seed, index=0, isTrain=isTrain)\n",
    "    \n",
    "    pooling_layer_0 = max_pooling_layer(input_x=conv_layer_0.output(),\n",
    "                                        k_size=pooling_size[0],\n",
    "                                        padding=\"VALID\")\n",
    "    \n",
    "    conv_layer_1 = conv_layer(input_x=pooling_layer_0.output(),\n",
    "                              in_channel=conv_featmap[0],\n",
    "                              out_channel=conv_featmap[1],\n",
    "                              kernel_shape=conv_kernel_size[0],\n",
    "                              rand_seed=seed, index=1, isTrain=isTrain)\n",
    "    \n",
    "    pooling_layer_1 = max_pooling_layer(input_x=conv_layer_1.output(),\n",
    "                                        k_size=pooling_size[1],\n",
    "                                        padding=\"VALID\")\n",
    "    \n",
    "    conv_layer_2 = conv_layer(input_x=pooling_layer_1.output(),\n",
    "                              in_channel=conv_featmap[1],\n",
    "                              out_channel=conv_featmap[2],\n",
    "                              kernel_shape=conv_kernel_size[0],\n",
    "                              rand_seed=seed, index=2, isTrain=isTrain)\n",
    "\n",
    "    pooling_layer_2 = max_pooling_layer(input_x=conv_layer_2.output(),\n",
    "                                        k_size=pooling_size[2],\n",
    "                                        padding=\"VALID\")\n",
    "\n",
    "    # flatten\n",
    "    pool_shape = pooling_layer_1.output().get_shape()\n",
    "    img_vector_length = pool_shape[1].value * pool_shape[2].value * pool_shape[3].value\n",
    "    flatten = tf.reshape(pooling_layer_1.output(), shape=[-1, img_vector_length])\n",
    "\n",
    "    #dropout layer\n",
    "    flatten_drop_0 = tf.nn.dropout(flatten, keep_prob)\n",
    "    # fc layer\n",
    "    fc_layer_0 = fc_layer(input_x=flatten_drop_0,\n",
    "                          in_size=img_vector_length,\n",
    "                          out_size=fc_units[0],\n",
    "                          rand_seed=seed,\n",
    "                          activation_function=tf.nn.relu,\n",
    "                          index=0, isTrain=isTrain)\n",
    "    flatten_drop_1 = tf.nn.dropout(fc_layer_0.output(), keep_prob)\n",
    "    \n",
    "    fc_layer_1 = fc_layer(input_x=flatten_drop_1,\n",
    "                          in_size=fc_units[0],\n",
    "                          out_size=fc_units[1],\n",
    "                          rand_seed=seed,\n",
    "                          activation_function=tf.nn.relu,\n",
    "                          index=1, isTrain=isTrain)\n",
    "    \n",
    "    flatten_drop_2 = tf.nn.dropout(fc_layer_1.output(), keep_prob)\n",
    "    fc_layer_2 = fc_layer(input_x=flatten_drop_2,\n",
    "                          in_size=fc_units[1],\n",
    "                          out_size=output_size,\n",
    "                          rand_seed=seed,\n",
    "                          activation_function=None,\n",
    "                          index=2, isTrain=isTrain)\n",
    "\n",
    "    # saving the parameters for l2_norm loss\n",
    "    conv_w = [conv_layer_0.weight, conv_layer_1.weight, conv_layer_2.weight]\n",
    "    fc_w = [fc_layer_0.weight, fc_layer_1.weight, fc_layer_2.weight]\n",
    "\n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        l2_loss = tf.reduce_sum([tf.norm(w) for w in fc_w])\n",
    "        l2_loss += tf.reduce_sum([tf.norm(w, axis=[-2, -1]) for w in conv_w])\n",
    "\n",
    "        label = tf.one_hot(input_y, 10)\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=fc_layer_2.output()),\n",
    "            name='cross_entropy')\n",
    "        loss = tf.add(cross_entropy_loss, l2_norm * l2_loss, name='loss')\n",
    "\n",
    "        tf.summary.scalar('LeNet_loss', loss)\n",
    "\n",
    "    return fc_layer_2.output(), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(output, input_y):\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        label = tf.one_hot(input_y, 10)\n",
    "        ce = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=output))\n",
    "\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(loss, learning_rate=1e-3):\n",
    "    with tf.name_scope('train_step'):\n",
    "        #step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        step = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(loss)\n",
    "\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, input_y):\n",
    "    with tf.name_scope('evaluate'):\n",
    "        pred = tf.argmax(output, axis=1)\n",
    "        error_num = tf.count_nonzero(pred - input_y, name='error_num')\n",
    "        tf.summary.scalar('LeNet_error_num', error_num)\n",
    "    return error_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X_train, y_train, X_val, y_val, \n",
    "             conv_featmap=[6],\n",
    "             fc_units=[84],\n",
    "             conv_kernel_size=[5],\n",
    "             pooling_size=[2],\n",
    "             l2_norm=0.01,\n",
    "             seed=235,\n",
    "             learning_rate=1e-2,\n",
    "             epoch=20,\n",
    "             batch_size=245,\n",
    "             keep_prob=1,\n",
    "             verbose=False,\n",
    "             pre_trained_model=None):\n",
    "    print(\"Building my LeNet. Parameters: \")\n",
    "    print(\"conv_featmap={}\".format(conv_featmap))\n",
    "    print(\"fc_units={}\".format(fc_units))\n",
    "    print(\"conv_kernel_size={}\".format(conv_kernel_size))\n",
    "    print(\"pooling_size={}\".format(pooling_size))\n",
    "    print(\"l2_norm={}\".format(l2_norm))\n",
    "    print(\"seed={}\".format(seed))\n",
    "    print(\"learning_rate={}\".format(learning_rate))\n",
    "\n",
    "    # define the variables and parameter needed during training\n",
    "    with tf.name_scope('inputs'):\n",
    "        xs = tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)\n",
    "        ys = tf.placeholder(shape=[None, ], dtype=tf.int64)\n",
    "    \n",
    "    isTrain = tf.placeholder(tf.bool, shape=())\n",
    "    output, loss = LeNet(xs, ys,\n",
    "                         img_len=32,\n",
    "                         channel_num=3,\n",
    "                         output_size=10,\n",
    "                         conv_featmap=conv_featmap,\n",
    "                         fc_units=fc_units,\n",
    "                         conv_kernel_size=conv_kernel_size,\n",
    "                         pooling_size=pooling_size,\n",
    "                         keep_prob=keep_prob,\n",
    "                         l2_norm=l2_norm,\n",
    "                         isTrain = isTrain,\n",
    "                         seed=seed)\n",
    "\n",
    "    iters = int(X_train.shape[0] / batch_size)\n",
    "    print('number of batches for training: {}'.format(iters))\n",
    "\n",
    "    step = train_step(loss)\n",
    "    eve = evaluate(output, ys)\n",
    "\n",
    "    iter_total = 0\n",
    "    best_acc = 0\n",
    "    cur_model_name = 'lenet_{}'.format(int(time.time()))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        merge = tf.summary.merge_all()\n",
    "\n",
    "        writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # try to restore the pre_trained\n",
    "        if pre_trained_model is not None:\n",
    "            try:\n",
    "                print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "                saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "            except Exception:\n",
    "                print(\"Load model Failed!\")\n",
    "                pass\n",
    "\n",
    "        for epc in range(epoch):\n",
    "            print(\"epoch {} \".format(epc + 1))\n",
    "\n",
    "            for itr in range(iters):\n",
    "                iter_total += 1\n",
    "\n",
    "                training_batch_x = X_train[itr * batch_size: (1 + itr) * batch_size]\n",
    "                training_batch_y = y_train[itr * batch_size: (1 + itr) * batch_size]\n",
    "\n",
    "                _, cur_loss = sess.run([step, loss], feed_dict={xs: training_batch_x, ys: training_batch_y, isTrain:True})\n",
    "\n",
    "                if iter_total % 100 == 0:\n",
    "                    # do validation\n",
    "                    valid_eve, merge_result = sess.run([eve, merge], feed_dict={xs: X_val, ys: y_val, isTrain:False})\n",
    "                    valid_acc = 100 - valid_eve * 100 / y_val.shape[0]\n",
    "                    if verbose:\n",
    "                        print('{}/{} loss: {} validation accuracy : {}%'.format(\n",
    "                            batch_size * (itr + 1),\n",
    "                            X_train.shape[0],\n",
    "                            cur_loss,\n",
    "                            valid_acc))\n",
    "\n",
    "                    # save the merge result summary\n",
    "                    writer.add_summary(merge_result, iter_total)\n",
    "\n",
    "                    # when achieve the best validation accuracy, we store the model paramters\n",
    "                    if valid_acc > best_acc:\n",
    "                        print('Best validation accuracy! iteration:{} accuracy: {}%'.format(iter_total, valid_acc))\n",
    "                        best_acc = valid_acc\n",
    "                        saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "\n",
    "    print(\"Traning ends. The best valid accuracy is {}. Model named {}.\".format(best_acc, cur_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building my LeNet. Parameters: \n",
      "conv_featmap=[32, 64, 96]\n",
      "fc_units=[500, 500]\n",
      "conv_kernel_size=[3, 3, 3]\n",
      "pooling_size=[2, 2, 2]\n",
      "l2_norm=0.0001\n",
      "seed=235\n",
      "learning_rate=0.001\n",
      "number of batches for training: 245\n",
      "epoch 1 \n",
      "Best validation accuracy! iteration:100 accuracy: 49.2%\n",
      "Best validation accuracy! iteration:200 accuracy: 53.9%\n",
      "epoch 2 \n",
      "Best validation accuracy! iteration:300 accuracy: 60.1%\n",
      "Best validation accuracy! iteration:400 accuracy: 60.7%\n",
      "epoch 3 \n",
      "Best validation accuracy! iteration:500 accuracy: 62.7%\n",
      "epoch 4 \n",
      "epoch 5 \n",
      "Best validation accuracy! iteration:1000 accuracy: 64.6%\n",
      "epoch 6 \n",
      "Best validation accuracy! iteration:1300 accuracy: 65.0%\n",
      "epoch 7 \n",
      "Best validation accuracy! iteration:1700 accuracy: 66.3%\n",
      "epoch 8 \n",
      "epoch 9 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a9a540a22222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m          \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m          pre_trained_model=None)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-34657b81c1f1>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(X_train, y_train, X_val, y_val, conv_featmap, fc_units, conv_kernel_size, pooling_size, l2_norm, seed, learning_rate, epoch, batch_size, keep_prob, verbose, pre_trained_model)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtraining_batch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_batch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_batch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misTrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0miter_total\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "training(X_train, y_train, X_val, y_val, \n",
    "         conv_featmap=[32,64,96],\n",
    "         fc_units=[500, 500],\n",
    "         conv_kernel_size=[3,3,3],\n",
    "         pooling_size=[2,2,2],\n",
    "         l2_norm=0.0001,\n",
    "         seed=235,\n",
    "         learning_rate=1e-3,\n",
    "         epoch=20,\n",
    "         batch_size=200,\n",
    "         keep_prob=0.8,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
